{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNiPst48se5lZuVt2ryuFJw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aKeles001/RNN_Large_Text_Generator/blob/main/RNN_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckqMDQgsHSaZ",
        "outputId": "a31c1673-fad3-4e05-d66d-5af0469e24f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "F-eGAJF7HgJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB2LdWkFHvNE",
        "outputId": "62c80763-d9ea-4669-b21d-cf45fc2d4dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip44ybWSAXQl",
        "outputId": "8efbd9db-f51a-4183-9f80-ce4b6d1d216c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode and Decode"
      ],
      "metadata": {
        "id": "JTljlTA7Ie17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "lICNIksTH43t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwi9ribhAd9F",
        "outputId": "db5b97f9-f502-45bf-9111-db704f5eaac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQf21WJzIR20",
        "outputId": "b73d2537-e740-416b-b25d-f86b49f2b152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Examples\n"
      ],
      "metadata": {
        "id": "1M9DJMPlIkNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "FxcOEBFCIjoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "W2_Wb9vAIqlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "JV4It5qHI7kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XJGweWe-AppN",
        "outputId": "a6ffdc49-f45f-4b85-a64a-af241366e244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "rv5pglDAJH1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Model"
      ],
      "metadata": {
        "id": "PHjmKk9MJUUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                            ),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "D7fZFpEEJPPe",
        "outputId": "e766478e-33b1-4bc7-f136-a91e54dace32",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Func"
      ],
      "metadata": {
        "id": "RLnJvEmPLQ5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is7yN3ejJjvO",
        "outputId": "322a84ca-6c65-4ff7-d858-21e90b9c4389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfojwInMA1Nq",
        "outputId": "d6e1d5cc-eaae-4cd0-93ea-ca86141a2ea5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 3.54651408e-03  1.54193572e-03  1.19518558e-03 ... -2.46529467e-04\n",
            "    2.26686694e-04  3.22777103e-03]\n",
            "  [ 2.41625425e-03  2.75857281e-03 -1.06388668e-03 ...  1.86652504e-03\n",
            "   -2.60631088e-03  6.72125840e-04]\n",
            "  [-1.51333792e-04  3.77886230e-03 -8.17798718e-04 ...  2.91840383e-03\n",
            "   -2.98248674e-03 -2.85943737e-03]\n",
            "  ...\n",
            "  [-5.85086411e-03  8.90022889e-03 -5.18945977e-04 ...  3.58937454e-04\n",
            "   -3.37513094e-03  4.25132457e-03]\n",
            "  [-3.82212456e-03  7.61138648e-03  6.78765308e-03 ...  2.17179488e-03\n",
            "   -3.91298300e-03  7.46537745e-03]\n",
            "  [-5.19871106e-03  1.60845066e-03  7.08152261e-03 ...  3.52965691e-03\n",
            "    9.19601298e-04  1.07164085e-02]]\n",
            "\n",
            " [[-8.14792176e-04 -2.46087392e-03  5.51923364e-03 ...  1.88936421e-03\n",
            "    3.92983388e-03 -3.48628592e-03]\n",
            "  [-5.92923164e-03  3.28129099e-05  1.59741484e-03 ... -9.41372477e-04\n",
            "    1.55959267e-03 -3.30618327e-03]\n",
            "  [-9.65873431e-03  2.17948924e-03 -9.92945395e-04 ... -2.57611671e-03\n",
            "    3.37818637e-04 -3.38079291e-03]\n",
            "  ...\n",
            "  [-2.90285470e-03  7.46264029e-03 -7.08318222e-03 ... -5.79578278e-04\n",
            "   -4.17194655e-03  1.30244205e-03]\n",
            "  [-6.58152811e-03  7.79051194e-03 -6.51354343e-03 ...  3.14852013e-03\n",
            "   -3.54389893e-03 -3.34867579e-03]\n",
            "  [-6.72007492e-03  7.03452155e-03 -5.17795468e-03 ...  4.57067695e-03\n",
            "   -3.76449293e-03 -5.99351246e-03]]\n",
            "\n",
            " [[ 1.42653659e-03  2.78479327e-03  6.00895844e-03 ... -1.62934477e-03\n",
            "    1.76885223e-04 -1.30048720e-03]\n",
            "  [ 1.76682835e-03  4.99839056e-03  1.07564544e-02 ... -3.29731917e-03\n",
            "    2.88337556e-04 -1.65611564e-03]\n",
            "  [-2.94116815e-03  5.89323277e-03  7.78460316e-03 ... -3.13007226e-03\n",
            "   -1.65673764e-03 -5.19886008e-03]\n",
            "  ...\n",
            "  [-5.34150004e-03 -8.92950804e-04 -7.48964585e-03 ...  3.18654953e-03\n",
            "    6.17337413e-03  3.57020035e-04]\n",
            "  [ 2.24722154e-03 -1.24582963e-03 -1.97636033e-03 ...  9.51685105e-03\n",
            "    5.85727813e-03  3.38961044e-03]\n",
            "  [ 5.68133686e-03 -5.20835747e-05 -6.44564687e-04 ...  6.67157955e-03\n",
            "    4.44322685e-03  5.20496303e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.40069274e-03  2.26675486e-03 -8.62671295e-04 ...  2.26640026e-03\n",
            "   -2.02894400e-04 -3.32980300e-03]\n",
            "  [-4.39528422e-03  2.07768125e-03 -7.83873315e-04 ... -9.22612613e-04\n",
            "    4.33526188e-03 -6.16958272e-03]\n",
            "  [ 4.54390887e-04  4.33281390e-03  4.95386121e-05 ... -5.58520958e-04\n",
            "    2.27670185e-03 -1.65110058e-03]\n",
            "  ...\n",
            "  [-2.28297315e-03  1.37368497e-03  5.79787325e-03 ...  2.96547543e-03\n",
            "    1.43337448e-03  1.14795640e-02]\n",
            "  [-5.31313010e-03  1.69305038e-03  2.36698054e-03 ...  3.47489002e-03\n",
            "   -1.78170856e-03  4.69803531e-03]\n",
            "  [-8.58333521e-03  2.02125008e-03 -1.60474970e-03 ...  9.07299167e-04\n",
            "   -2.63838517e-03  4.18695854e-03]]\n",
            "\n",
            " [[-3.39295692e-03  3.55415686e-04 -5.08463709e-04 ... -2.42949091e-03\n",
            "    4.65492392e-03 -4.21011355e-03]\n",
            "  [-1.40279287e-03  3.21877538e-03 -5.89650543e-03 ... -3.48653179e-03\n",
            "    6.42375508e-03  1.14567127e-04]\n",
            "  [ 7.46455044e-05  5.04965242e-03 -8.17116164e-03 ... -1.70547108e-03\n",
            "    4.94973920e-03  2.22238526e-03]\n",
            "  ...\n",
            "  [-3.46456491e-03 -1.75860093e-03  6.21307641e-04 ...  7.77023286e-03\n",
            "    3.05935787e-03  2.88484240e-04]\n",
            "  [-2.71645753e-04  6.47133391e-04  2.70348531e-03 ...  7.11647654e-03\n",
            "    5.57344640e-04  4.13031579e-04]\n",
            "  [-1.73353241e-03 -2.75961426e-03  7.82092940e-03 ...  7.58415088e-03\n",
            "    3.15469201e-03 -5.13279997e-03]]\n",
            "\n",
            " [[-1.40069274e-03  2.26675486e-03 -8.62671295e-04 ...  2.26640026e-03\n",
            "   -2.02894400e-04 -3.32980300e-03]\n",
            "  [ 3.11487296e-04  3.20423814e-03 -4.25659958e-03 ...  2.22125836e-03\n",
            "    1.40390813e-03  3.98172706e-05]\n",
            "  [-3.16188298e-03 -2.15833611e-03 -8.51340219e-03 ...  1.43074791e-03\n",
            "    5.42364735e-03 -8.38924258e-04]\n",
            "  ...\n",
            "  [-3.68421315e-03  7.30063766e-04  5.92748460e-04 ... -9.49775567e-04\n",
            "    6.75051007e-03  6.44631917e-03]\n",
            "  [-5.49137965e-03  2.15546647e-03  8.33125843e-04 ...  1.52291160e-03\n",
            "    4.10316093e-03  5.32479608e-04]\n",
            "  [-5.01307566e-03  2.52135680e-03  1.45790435e-03 ...  5.40589541e-03\n",
            "    4.07112902e-03  7.98020978e-04]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhwdzDHgOccP",
        "outputId": "a754a0ef-be16-4d18-a090-d7ed9f64b2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time = pred[0]\n",
        "print(len(time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk55VRREOhNM",
        "outputId": "1603532d-3cfa-4248-deb7-99b6b5b5a1ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_pred = pred[0]\n",
        "print(len(time_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgp-txoaO7xi",
        "outputId": "72438bbf-4e42-4d4e-9e38-808305e25c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZoT44aGTPIdX",
        "outputId": "4e9230b0-773a-4472-fcd6-40e2d15d2468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!s!BZi&snBAEAfJTOyHDLbzxQvpQTDgAAZBjou:\\nu-lAjPazfhhpcT.EzMAo& ;vNn!ft?ey':FRfsXljiPXUfi$YAkzq-:'.'X;\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable(package=\"my_custom_package\")\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "_sfPzu7kPUFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "u9Q5uc1DPoX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "CXFVPrQ8Pq05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "vNwHHmbWQEHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=4, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d20-YUq-Pw2X",
        "outputId": "b2b2d442-90a2-4a98-a165-f5749533c57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - loss: 2.0077\n",
            "Epoch 2/4\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 74ms/step - loss: 1.6503\n",
            "Epoch 3/4\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 74ms/step - loss: 1.4958\n",
            "Epoch 4/4\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - loss: 1.4132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "  num_generate = 800\n",
        "\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      predictions = predictions[0]\n",
        "\n",
        "\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "E7hwpvXWQqv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK6NWfjWRCgn",
        "outputId": "d2a21f2e-5d4d-49a0-d004-5ad838ab4d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: Night\n",
            "Night Turks from Lie'd tender dield\n",
            "With God and gentlemen, whach knoss thy love as if\n",
            "it dainy and confemence.\n",
            "\n",
            "LUCIO:\n",
            "Woo alone;\n",
            "Which hath be or but into the insterrips?\n",
            "Awedest me, to your words cannot he!\n",
            "there are it more than his are maids of more love.\n",
            "\n",
            "Second Tomfort.\n",
            "\n",
            "TLANDIO:\n",
            "I'll go, lorgs intermonity. I will not meet\n",
            "By love forly as will have yet might behoved for you no merry with sight-hours\n",
            "and fords and woe, help's tritake, let's the friends,\n",
            "Sidos, means: for look'd like the bury me'n.\n",
            "\n",
            "SEBASTIAN:\n",
            "I swear thee, way tell fortunus and commints\n",
            "Sift it tears. But you, night like a cursfail,\n",
            "That's a traitor him mojes of the mal old ewn loves, bere advertion'd.\n",
            "Though not yet shall be condinted host wan herselves.\n",
            "\n",
            "VOLUMNIA:\n",
            "Well were you.'\n",
            "\n",
            "LEONTES:\n",
            "This, and lets, then meancour\n"
          ]
        }
      ]
    }
  ]
}